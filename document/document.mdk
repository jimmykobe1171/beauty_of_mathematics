[INCLUDE=presentation]
Title         : Beauty of mathematics
Author        : Jimmy
Email         : jimmy@motherapp.com
Reveal Theme  : sky
Beamer Theme  : singapore
Package: gensymb

[TITLE]

# Content

[TOC]

## Nature Language Processing

At first, researchers use `grammar rules` to parse a sentence.
![Screen Shot 2016-03-01 at 10.40.26 PM]

[Screen Shot 2016-03-01 at 10.40.26 PM]: images/Screen-Shot-2016-03-01-at-10.40.26-PM.png "Screen Shot 2016-03-01 at 10.40.26 PM" { width:auto; max-width:90% }

Drawbacks:

* huge computational complexity
* hard to cover all grammar rules.


## Statistical Language Model

Nowadays, we use mathematics, or statistics more precisely.

$S = w_1,w_2,...,w_n$

~ Equation
P(S) = P(w_1)*P(w_2|w_1)...P(w_n|w_1,w_2,...w_{n-1})
~
using [Markov Assumption](https://en.wikipedia.org/wiki/Markov_property):

~ Equation {#bigram_model}
P(S) = P(w_1)*P(w_2|w_1)...P(w_n|w_{n-1})
~
Equation [#bigram_model] is called **Bigram Model**.

## Hidden Markov Model
![Screen Shot 2016-03-02 at 10.12.18 PM]

[Screen Shot 2016-03-02 at 10.12.18 PM]: images/Screen-Shot-2016-03-02-at-10.12.18-PM.png "Screen Shot 2016-03-02 at 10.12.18 PM" { width:auto; max-width:90% }

What the hell could such thing do ?


~ Slide
### Applications

* [speech recognition](https://www.ll.mit.edu/publications/journal/pdf/vol03_no1/3.1.3.speechrecognition.pdf)
* [stock prediction](http://arxiv.org/pdf/1311.4771.pdf)
* [bioinformatics](http://www.bioss.ac.uk/people/dirk/talks/tutorial_hmm_bioinf.pdf)

~

~ Slide
### Hidden Markov Model in Speech Recognition

~

## Law of Cosines and News Categorization
![Screen Shot 2016-03-03 at 上午11.28.00]

[Screen Shot 2016-03-03 at 上午11.28.00]: images/Screen-Shot-2016-03-03-at-11.28.00.png "Screen Shot 2016-03-03 at 上午11.28.00" { width:auto; max-width:90% }

~ Equation
\cos\theta=\frac{\vec{a}\cdot\vec{b}}{|a||b|}
~

~ Slide
### Represent News Using Feature Vector

**TF-IDF**(Term Frequency/Inverse Document Frequency)

For example: \
a website contains 1000 words. word "math" occured 30 times. Assume there are 20000 websites in Internet, and 200 of them
contains word "math".

* TF: 30/1000
* IDF: $log(\frac{20000}{200})$

Then, word "math"'s TF-IDF value = $TF\cdot IDF$ 
~

~ Slide
### Construct Feature Vectors

Say we have an vocabulary list(50000 words) $(w_1, w_2,...w_50000)$. let $w_i$ represent the `ith` word, `tf-idf()` represent a function that
calculate a word's TF-IDF.

For each word in the vocabulary list, we can calculate it's TF-IDF. Then, we can get an vector of 50000 TF-IDF values:

$[tf-idf(w_1), tf-idf(w_2),..., tf-idf(w_{50000})]$

Now we successfully change a website into an vector, also called **feature vector**.
~

~ Slide


### Calculate Similarity

Say we have two websites's feature vectors: $v_1$ and $v_2$.

we can use `law of consines` to compute the angle of the two vectors:

$\cos\theta=\frac{\vec{v_1}\cdot\vec{v_2}}{|v_1||v_2|}$

if $\theta=0\degree$, it means the two website have high similarity.
~

## Viterbi and Viterbi Algorithm

The Viterbi algorithm is a dynamic programming algorithm for finding the most likely sequence of hidden states
in Hidden Markov Model, proposed by [Andrew Viterbi](https://en.wikipedia.org/wiki/Andrew_Viterbi).


~ Slide
### Chinese Input Method (拼音输入法)

when we input a series of T characters $y_1,y_2,...,y_T$, we get the corresponding T Chinese word $x_1,x_2,...,x_T$.
This is a Hidden Markov Model

![Screen Shot 2016-03-02 at 10.12.18 PM]

[Screen Shot 2016-03-02 at 10.12.18 PM]: images/Screen-Shot-2016-03-02-at-10.12.18-PM.png "Screen Shot 2016-03-02 at 10.12.18 PM" { width:auto; max-width:90% }

~

~ Slide
### Chinese Input Method (拼音输入法)

Input characters $y_1,y_2,...,y_T$ is the observations, Chinese word $x_1,x_2,...,x_T$ is the hidden variables.

![Screen Shot 2016-03-02 at 10.12.18 PM]

The problem is how to find out the most likely combinations $x_1,x_2,...,x_T$. Assume we know the Hidden Markov Model already,
say state transition probabilities $P(x_{i+1}|x_i)$ and output probabilities $P(y_i|x_i)$.
~

~ Slide
### Chinese Input Method (拼音输入法)

The basic solution is to try out all the combinations, for every combinations, we calculate its probability:
$P(x_1,x_2,...,x_T|y_1,y_2,...,y_T) = \prod_{i=1}^{T}P(y_i|x_i)\cdot P(x_i|x_{i-1})$

However, the computational complexity of this method is $O(N^T)$, where N is the possible
state of variable X.

![Screen Shot 2016-03-07 at 9.14.28 PM]

[Screen Shot 2016-03-07 at 9.14.28 PM]: images/Screen-Shot-2016-03-07-at-9.14.28-PM.png "Screen Shot 2016-03-07 at 9.14.28 PM" { width:auto; max-width:90% }

~

~ Slide
### Viterbi Algorithm
Let's do some approximations:

if a sentence contains 10 words, one letter can have 13 possible corresponding Chinese words. Then there are
$13^{10}\approx5\times10^{14}$ cases. If each case need 20 multiple operations. the computational complexity is about $O(10^{16})$.
Approximately 1 day for computer to get the result.

~

~ Slide
### Viterbi Algorithm
There are a lot of unnecessary computations. By using [Dynamic Programming](https://en.wikipedia.org/wiki/Dynamic_programming),
we can decrease the computational complexity to $O(N^2 \cdot T)$. In previous case, it is
$13\times13\times10\approx10^3$.
![Screen Shot 2016-03-07 at 9.14.28 PM]

~

## Thanks for looking :-)